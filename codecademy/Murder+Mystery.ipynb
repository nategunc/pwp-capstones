{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble: A Brand New Jay\n",
    "\n",
    "After an eventful season on season 8 of *A Brand New Jay*, the 3 remaining contestants were invited to Jay Stacksby's private island for the last three episodes. When the day of filming the finale came Mr. Stacksby was found with one of his Professional Series 8-inch Chef Knives plunged through his heart! After the initial investigation highlighted that the film crew all lived in a separate house on the other side of the island, it was concluded that only the three contestants were near enough to Stacksby in order to commit a crime. At the scene of the crime, a letter was left. Here are the contents of that letter:\n",
    "\n",
    "> You may call me heartless, a killer, a monster, a murderer, but I'm still NOTHING compared to the villian that Jay was. This whole contest was a sham, an elaborate plot to shame the contestants and feed Jay's massive, massive ego. SURE you think you know him! You've seen him smiling for the cameras, laughing, joking, telling stories, waving his money around like a prop but off camera he was a sinister beast, a cruel cruel taskmaster, he treated all of us like slaves, like cattle, like animals! Do you remember Lindsay, she was the first to go, he called her such horrible things that she cried all night, keeping up all up, crying, crying, and more crying, he broke her with his words. I miss my former cast members, all of them very much. And we had to live with him, live in his home, live in his power, deal with his crazy demands. AND FOR WHAT! DID YOU KNOW THAT THE PRIZE ISN'T REAL? He never intended to marry one of us! The carrot on the stick was gone, all that was left was stick, he told us last night that we were all a terrible terrible disappointment and none of us would ever amount to anything, and that regardless of who won the contest he would never speak to any of us again! It's definitely the things like this you can feel in your gut how wrong he is! Well I showed him, he got what he deserved all right, I showed him, I showed him the person I am! I wasn't going to be pushed around any longer, and I wasn't going to let him go on pretending that he was some saint when all he was was a sick sick twisted man who deserved every bit of what he got. The fans need to know, Jay Stacksby is a vile amalgamation of all things evil and bad and the world is a better place without him.\n",
    "\n",
    "Pretty sinister stuff! Luckily, in addition to this bold-faced admission, we have the introduction letters of the three contestants. Maybe there is a way to use this information to determine who the author of this murder letter is?\n",
    "\n",
    "Myrtle Beech's introduction letter:\n",
    "> Salutations. My name? Myrtle. Myrtle Beech. I am a woman of simple tastes. I enjoy reading, thinking, and doing my taxes. I entered this competition because I want a serious relationship. I want a commitment. The last man I dated was too whimsical. He wanted to go on dates that had no plan. No end goal. Sometimes we would just end up wandering the streets after dinner. He called it a \"walk\". A \"walk\" with no destination. Can you imagine? I like every action I take to have a measurable effect. When I see a movie, I like to walk away with insights that I did not have before. When I take a bike ride, there better be a worthy destination at the end of the bike path. Jay seems frivolous at times. This worries me. However, it is my staunch belief that one does not make and keep money without having a modicum of discipline. As such, I am hopeful. I will now list three things I cannot live without. Water. Emery boards. Dogs. Thank you for the opportunity to introduce myself. I look forward to the competition. \n",
    "\n",
    "Lily Trebuchet's introduction letter:\n",
    "> Hi, I'm Lily Trebuchet from East Egg, Long Island. I love cats, hiking, and curling up under a warm blanket with a book. So they gave this little questionnaire to use for our bios so lets get started. What are some of my least favorite household chores? Dishes, oh yes it's definitely the dishes, I just hate doing them, don't you? Who is your favorite actor and why? Hmm, that's a hard one, but I think recently I'll have to go with Michael B. Jordan, every bit of that man is handsome, HANDSOME! Do you remember seeing him shirtless? I can't believe what he does for the cameras! Okay okay next question, what is your perfect date? Well it starts with a nice dinner at a delicious but small restaurant, you know like one of those places where the owner is in the back and comes out to talk to you and ask you how your meal was. My favorite form of art? Another hard one, but I think I'll have to go with music, music you can feel in your whole body and it is electrifying and best of all, you can dance to it! Okay final question, let's see, What are three things you cannot live without? Well first off, my beautiful, beautiful cat Jerry, he is my heart and spirit animal. Second is pasta, definitely pasta, and the third I think is my family, I love all of them very much and they support me in everything I do. I know Jay Stacksby is a handsome man and all of us want to be the first to walk down the aisle with him, but I think he might truly be the one for me. Okay that's it for the bio, I hope you have fun watching the show! \n",
    "\n",
    "Gregg T Fishy's introduction letter:\n",
    "\n",
    "> A good day to you all, I am Gregg T Fishy, of the Fishy Enterprise fortune. I am 37 years young. An adventurous spirit and I've never lost my sense of childlike wonder. I do love to be in the backyard gardening and I have the most extraordinary time when I'm fishing. Fishing for what, you might ask? Why, fishing for compliments of course! I have a stunning pair of radiant blue eyes. They will pierce the soul of anyone who dare gaze upon my countenance. I quite enjoy going on long jaunts through garden paths and short walks through greenhouses. I hope that Jay will be as absolutely interesting as he appears on the television. I find that he has some of the most curious tastes in style and humor. When I'm out and about I quite enjoy hearing tales that instill in my heart of hearts the fascination that beguiles my every day life. Every fiber of my being scintillates and vascillates with extreme pleasure during one of these charming anecdotes and significantly pleases my beautiful personage. I cannot wait to enjoy being on A Brand New Jay. It certainly seems like a grand time to explore life and love."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving The Different Examples as Variables\n",
    "\n",
    "First let's create variables to hold the text data in! Save the muder note as a string in a variable called `murder_note`. Save Lily Trebuchet's introduction into `lily_trebuchet_intro`. Save Myrtle Beech's introduction into `myrtle_beech_intro`. Save Gregg T Fishy's introduction into `gregg_t_fishy_intro`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating variables containing text strings from murder note and suspect bios\n",
    "murder_note = '''You may call me heartless, a killer, a monster, a murderer, but I'm still NOTHING compared to the villian that Jay was. This whole contest was a sham, an elaborate plot to shame the contestants and feed Jay's massive, massive ego. SURE you think you know him! You've seen him smiling for the cameras, laughing, joking, telling stories, waving his money around like a prop but off camera he was a sinister beast, a cruel cruel taskmaster, he treated all of us like slaves, like cattle, like animals! Do you remember Lindsay, she was the first to go, he called her such horrible things that she cried all night, keeping up all up, crying, crying, and more crying, he broke her with his words. I miss my former cast members, all of them very much. And we had to live with him, live in his home, live in his power, deal with his crazy demands. AND FOR WHAT! DID YOU KNOW THAT THE PRIZE ISN'T REAL? He never intended to marry one of us! The carrot on the stick was gone, all that was left was stick, he told us last night that we were all a terrible terrible disappointment and none of us would ever amount to anything, and that regardless of who won the contest he would never speak to any of us again! It's definitely the things like this you can feel in your gut how wrong he is! Well I showed him, he got what he deserved all right, I showed him, I showed him the person I am! I wasn't going to be pushed around any longer, and I wasn't going to let him go on pretending that he was some saint when all he was was a sick sick twisted man who deserved every bit of what he got. The fans need to know, Jay Stacksby is a vile amalgamation of all things evil and bad and the world is a better place without him.'''\n",
    "lily_trebuchet_intro = '''Hi, I'm Lily Trebuchet from East Egg, Long Island. I love cats, hiking, and curling up under a warm blanket with a book. So they gave this little questionnaire to use for our bios so lets get started. What are some of my least favorite household chores? Dishes, oh yes it's definitely the dishes, I just hate doing them, don't you? Who is your favorite actor and why? Hmm, that's a hard one, but I think recently I'll have to go with Michael B. Jordan, every bit of that man is handsome, HANDSOME! Do you remember seeing him shirtless? I can't believe what he does for the cameras! Okay okay next question, what is your perfect date? Well it starts with a nice dinner at a delicious but small restaurant, you know like one of those places where the owner is in the back and comes out to talk to you and ask you how your meal was. My favorite form of art? Another hard one, but I think I'll have to go with music, music you can feel in your whole body and it is electrifying and best of all, you can dance to it! Okay final question, let's see, What are three things you cannot live without? Well first off, my beautiful, beautiful cat Jerry, he is my heart and spirit animal. Second is pasta, definitely pasta, and the third I think is my family, I love all of them very much and they support me in everything I do. I know Jay Stacksby is a handsome man and all of us want to be the first to walk down the aisle with him, but I think he might truly be the one for me. Okay that's it for the bio, I hope you have fun watching the show!'''\n",
    "myrtle_beech_intro = '''Salutations. My name? Myrtle. Myrtle Beech. I am a woman of simple tastes. I enjoy reading, thinking, and doing my taxes. I entered this competition because I want a serious relationship. I want a commitment. The last man I dated was too whimsical. He wanted to go on dates that had no plan. No end goal. Sometimes we would just end up wandering the streets after dinner. He called it a \"walk\". A \"walk\" with no destination. Can you imagine? I like every action I take to have a measurable effect. When I see a movie, I like to walk away with insights that I did not have before. When I take a bike ride, there better be a worthy destination at the end of the bike path. Jay seems frivolous at times. This worries me. However, it is my staunch belief that one does not make and keep money without having a modicum of discipline. As such, I am hopeful. I will now list three things I cannot live without. Water. Emery boards. Dogs. Thank you for the opportunity to introduce myself. I look forward to the competition.'''\n",
    "gregg_t_fishy_intro = '''A good day to you all, I am Gregg T Fishy, of the Fishy Enterprise fortune. I am 37 years young. An adventurous spirit and I've never lost my sense of childlike wonder. I do love to be in the backyard gardening and I have the most extraordinary time when I'm fishing. Fishing for what, you might ask? Why, fishing for compliments of course! I have a stunning pair of radiant blue eyes. They will pierce the soul of anyone who dare gaze upon my countenance. I quite enjoy going on long jaunts through garden paths and short walks through greenhouses. I hope that Jay will be as absolutely interesting as he appears on the television. I find that he has some of the most curious tastes in style and humor. When I'm out and about I quite enjoy hearing tales that instill in my heart of hearts the fascination that beguiles my every day life. Every fiber of my being scintillates and vascillates with extreme pleasure during one of these charming anecdotes and significantly pleases my beautiful personage. I cannot wait to enjoy being on A Brand New Jay. It certainly seems like a grand time to explore life and love.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The First Indicator: Sentence Length\n",
    "\n",
    "Perhaps some meaningful data can first be gleaned from these text examples if we measure how long the average sentence length is. Different authors have different patterns of written speech, so this could be very useful in tracking down the killer.\n",
    "\n",
    "Write a function `get_average_sentence_length` that takes some `text` as an argument. This function should return the average number of words in a sentence in the text.\n",
    "\n",
    "Hint (highlight this hint in order to reveal it): \n",
    "<font color=\"white\">Use your knowledge of _string methods_ to create a list of all of the sentences in a text, called **sentences_in_text**. \n",
    "Further break up each **sentences_in_text** into a list of words and save the _length_ of that list of words to a new list that contains all the sentence lengths, called **sentence_lengths**. Take the average of all of the sentence lengths by adding them all together and dividing by the number of sentences (which should be the same as the length of the **sentence_lengths**).\n",
    "\n",
    "Remember sentences can end with more than one kind of punctuation, you might find it easiest to use **.replace()** so you only have to split on one punctuation mark. Remember **.replace()** doesn't modify the string itself, it returns a new string!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_sentence_length(text):\n",
    "    \n",
    "    #Begin by splitting text into sentences. Sentences can end in ./?/!\n",
    "    #Will first replace ? and ! with . and then split into sentnces based on .\n",
    "    text_replace_excl = text.replace('!', '.')\n",
    "    text_replace_excl_and_quest = text_replace_excl.replace('?', '.')\n",
    "    \n",
    "    #Create list containing each sentence in text\n",
    "    sentences_in_text = text_replace_excl_and_quest.split('.')\n",
    "    \n",
    "    #Create a list of words in each sentence\n",
    "    list_of_words = []\n",
    "    \n",
    "    i = 0 #Create iteration variable\n",
    "    \n",
    "    for sentence in sentences_in_text:\n",
    "        list_of_words.append(sentences_in_text[i].split())\n",
    "        i += 1\n",
    "    \n",
    "    #Create empty list containing the lengths of each sentence\n",
    "    sentence_lengths = []\n",
    "    \n",
    "    #Calculate the number of words in each sentence and append to list\n",
    "    for word in list_of_words:\n",
    "        sentence_lengths.append(len(word))     \n",
    "    \n",
    "    #Calculate the average sentence length\n",
    "    total_length = 0\n",
    "    for length in sentence_lengths:\n",
    "        total_length += length\n",
    "    average_length = total_length / len(sentence_lengths)\n",
    "    return average_length  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating The Definition for Our Model\n",
    "\n",
    "Now that we have a metric we want to save and data that is coupled with that metric, it might be time to create our data type. Let's define a class called `TextSample` with a constructor. The constructor should take two arguments: `text` and `author`. `text` should be saved as `self.raw_text`. Call `get_average_sentence_length` with the raw text and save it to `self.average_sentence_length`. You should save the author of the text as `self.author`.\n",
    "\n",
    "Additionally, define a string representation for the model. If you print a `TextSample` it should render:\n",
    " - The author's name\n",
    " - The average sentence length\n",
    " \n",
    "This will be your main class for the problem at hand. All later instruction to update `TextSample` should be done in the code block below. After updating `TextSample`, click on the `Cell` option in the Jupyter Notebook main menu above, then click `Run All` to rerun the cells from top to bottom. If you need to restart your Jupyter Notebook either run the cells below first or move the `TextSample` class definition & instantiation cells to the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSample:\n",
    "    def __init__ (self, text, author):\n",
    "        self.raw_text = text\n",
    "        self.author = author\n",
    "        self.average_sentence_length = get_average_sentence_length(self.raw_text)\n",
    "        self.prepared_text = prepare_text(text)\n",
    "        self.word_count_frequency = build_frequency_table(self.prepared_text)\n",
    "        self.ngram_frequency = build_frequency_table(ngram_creator(self.prepared_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our TextSample Instances\n",
    "\n",
    "Now create a `TextSample` object for each of the samples of text that we have.\n",
    " - `murderer_sample` for the murderer's note.\n",
    " - `lily_sample` for Lily Trebuchet's note.\n",
    " - `myrtle_sample` for Myrtle Beech's note.\n",
    " - `gregg_sample` for Gregg T Fishy's note.\n",
    " \n",
    "Print out each one after instantiating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.TextSample object at 0x0000000007C16C18>\n",
      "20.6875\n",
      "<__main__.TextSample object at 0x0000000007C16B38>\n",
      "15.05\n",
      "<__main__.TextSample object at 0x00000000032817B8>\n",
      "6.517241379310345\n",
      "<__main__.TextSample object at 0x000000000325AF98>\n",
      "12.625\n"
     ]
    }
   ],
   "source": [
    "#Instantiating Text Samples\n",
    "murderer_sample = TextSample(murder_note, \"Murderer\")\n",
    "lily_sample = TextSample(lily_trebuchet_intro, \"Lily Trebuchet\")\n",
    "myrtle_sample = TextSample(myrtle_beech_intro, \"Myrtle Beech\")\n",
    "gregg_sample = TextSample(gregg_t_fishy_intro, \"Gregg T Fishy\")\n",
    "\n",
    "#Printing Object and Average Sentence Length for Each Sample\n",
    "print(murderer_sample)\n",
    "print(murderer_sample.average_sentence_length)\n",
    "print(lily_sample)\n",
    "print(lily_sample.average_sentence_length)\n",
    "print(myrtle_sample)\n",
    "print(myrtle_sample.average_sentence_length)\n",
    "print(gregg_sample)\n",
    "print(gregg_sample.average_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Our Data\n",
    "\n",
    "We want to compare the word choice and usage between the samples, but sentences make our text data fairly messy. In order to analyze the different messages fairly, we'll need to remove all the punctuation and uppercase letters from the samples.\n",
    "\n",
    "Create a function called `prepare_text` that takes a single parameter `text`, makes the text entirely lowercase, removes all the punctuation and returns a list of the words in the text in order.\n",
    "\n",
    "For example: `\"Where did you go, friend? We nearly saw each other.\"` would become `['where', 'did', 'you', 'go', 'friend', 'we', 'nearly', 'saw', 'each', 'other']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text(text):\n",
    "    \n",
    "    #First stripping out punctuation\n",
    "    punctuation = \"!.?,\"\n",
    "    text_without_punctuation = \"\"\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            text_without_punctuation += char\n",
    "            \n",
    "    #Converting text string to all lower case\n",
    "    all_lower_no_punct_string = text_without_punctuation.lower()\n",
    "    \n",
    "    #Converting string to list of all words contained in the string\n",
    "    prepared_text = all_lower_no_punct_string.split()\n",
    "    return prepared_text  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the constructor for `TextSample` to save the prepared text as `self.prepared_text`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Frequency Table\n",
    "\n",
    "Now we want to see which words were most frequently used in each of the samples. Create a function called `build_frequency_table`. It takes in a list called `corpus` and creates a dictionary called `frequency_table`. For every element in `corpus` the value `frequency_table[element]` should be equal to the number of times that element appears in `corpus`. For example the input `['do', 'you', 'see', 'what', 'i', 'see']` would create the frequency table `{'what': 1, 'you': 1, 'see' 2, 'i': 1}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_frequency_table(corpus):\n",
    "    \n",
    "    # 1 - Create List of Unique Terms\n",
    "    unique_terms = []\n",
    "    for term in corpus:\n",
    "        if term not in unique_terms:\n",
    "            unique_terms.append(term)          \n",
    "    \n",
    "    # 2 - Create List Containing the Number of Times Each Term Occurs\n",
    "    num_occurrences = []\n",
    "    for unique_term in unique_terms:\n",
    "        count = 0\n",
    "        for term in corpus:\n",
    "            if unique_term == term:\n",
    "                count += 1\n",
    "        num_occurrences.append(count)\n",
    "    \n",
    "    # 3 - Use List Comprehension to combine these two into a dictionary\n",
    "    frequency_table = {key:value for key, value in zip(unique_terms, num_occurrences)} \n",
    "    return frequency_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Second Indicator: Favorite Words\n",
    "\n",
    "Use `build_frequency_table` with the prepared text to create a frequency table that counts how frequently all the words in each text sample appears. Call these functions in the constructor for `TextSample` and assign the word frequency table to a value called `self.word_count_frequency`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seen': 1, 'we': 2, 'anything': 1, 'crazy': 1, 'know': 3, \"you've\": 1, 'a': 11, 'told': 1, 'sinister': 1, 'without': 1, 'do': 1, 'waving': 1, 'you': 6, 'laughing': 1, 'called': 1, 'amount': 1, 'would': 2, 'were': 1, 'bad': 1, 'place': 1, 'none': 1, 'miss': 1, 'killer': 1, 'elaborate': 1, 'things': 3, 'showed': 3, 'and': 9, 'prop': 1, 'what': 3, 'person': 1, 'gut': 1, 'be': 1, 'left': 1, 'like': 5, 'her': 2, 'off': 1, 'an': 1, 'the': 12, 'terrible': 2, 'how': 1, 'ego': 1, 'remember': 1, 'feed': 1, 'prize': 1, 'sure': 1, 'villian': 1, 'night': 2, 'call': 1, 'let': 1, 'fans': 1, 'nothing': 1, 'cameras': 1, 'am': 1, 'i': 7, 'she': 2, 'right': 1, 'still': 1, 'every': 1, 'but': 2, 'world': 1, 'taskmaster': 1, 'words': 1, 'such': 1, 'intended': 1, 'treated': 1, 'disappointment': 1, 'plot': 1, \"jay's\": 1, 'animals': 1, 'contestants': 1, 'pushed': 1, 'cried': 1, 'had': 1, 'amalgamation': 1, 'when': 1, 'demands': 1, 'around': 2, 'again': 1, 'to': 10, 'monster': 1, 'them': 1, 'money': 1, 'feel': 1, 'camera': 1, 'massive': 2, 'more': 1, 'jay': 2, 'joking': 1, 'sick': 2, 'is': 3, 'members': 1, 'up': 2, 'murderer': 1, 'go': 2, 'shame': 1, 'for': 2, 'telling': 1, 'marry': 1, 'home': 1, \"i'm\": 1, 'whole': 1, 'with': 3, 'he': 13, 'cattle': 1, 'cruel': 2, 'may': 1, 'was': 10, 'that': 7, 'wrong': 1, 'stacksby': 1, 'on': 2, 'stick': 2, 'me': 1, 'carrot': 1, 'pretending': 1, 'who': 2, 'ever': 1, 'twisted': 1, 'bit': 1, 'crying': 3, 'much': 1, 'one': 1, 'compared': 1, 'some': 1, 'definitely': 1, 'horrible': 1, 'sham': 1, 'man': 1, 'us': 5, 'live': 3, 'in': 3, 'did': 1, 'well': 1, \"wasn't\": 2, 'better': 1, 'gone': 1, 'my': 1, 'his': 5, 'any': 2, 'won': 1, 'regardless': 1, 'first': 1, 'last': 1, 'your': 1, \"isn't\": 1, 'think': 1, 'slaves': 1, 'need': 1, 'evil': 1, 'saint': 1, 'heartless': 1, 'all': 9, 'power': 1, 'never': 2, 'speak': 1, 'of': 8, 'smiling': 1, 'very': 1, 'broke': 1, 'beast': 1, 'got': 2, 'former': 1, 'this': 2, 'going': 2, 'deserved': 2, 'deal': 1, 'real': 1, 'him': 8, \"it's\": 1, 'can': 1, 'contest': 2, 'vile': 1, 'keeping': 1, 'cast': 1, 'stories': 1, 'longer': 1, 'lindsay': 1}\n"
     ]
    }
   ],
   "source": [
    "#Viewing Frequency Table for Murder Note Text Sample\n",
    "print(murderer_sample.word_count_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Third Indicator: N-Grams\n",
    "\n",
    "An <a href='https://en.wikipedia.org/wiki/N-gram' target=\"_blank\">n-gram</a> is a text analysis technique used for pattern recognition and applicable throughout lingusitics. We're going to use n-grams to find who uses similar word-pairs to the murderer, and we think it's going to make our evidence strong enough to conclusively find the killer.\n",
    "\n",
    "Create a function called `ngram_creator` that takes a parameter `text_list`, a treated in-order list of the words in a text sample. `ngram_creator` should return a list of all adjacent pairs of words, styled as strings with a space in the center.\n",
    "\n",
    "For instance, calling `ngram_creator` with the input `['what', 'in', 'the', 'world', 'is', 'going', 'on']`\n",
    "Should produce the output `['what in', 'in the', 'the world', 'world is', 'is going', 'going on']`.\n",
    "\n",
    "These are two-word n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_creator(text_list):\n",
    "    \n",
    "    #Creating an empty list which will contain ngrams\n",
    "    ngram_list = []\n",
    "    \n",
    "    #Add ngrams to ngram_list\n",
    "    i = 0\n",
    "    for term in range(len(text_list) - 1):\n",
    "        ngram = text_list[i] + \" \" + text_list[i+1]\n",
    "        ngram_list.append(ngram)\n",
    "        i += 1\n",
    "    return(ngram_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `ngram_creator` along with the prepared text to create a list of all the two-word ngrams in each `TextSample`. Use `build_frequency_table` to tabulate the frequency of each ngram. In the constructor for `TextSample` save this frequency table as `self.ngram_frequency`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bad and': 1, 'money around': 1, 'know jay': 1, 'such horrible': 1, 'monster a': 1, 'twisted man': 1, 'ever amount': 1, 'was some': 1, 'much and': 1, 'feel in': 1, 'massive ego': 1, 'us like': 1, 'all right': 1, \"i'm still\": 1, 'a killer': 1, 'without him': 1, 'live with': 1, 'contestants and': 1, 'i am': 1, 'we had': 1, 'the person': 1, \"prize isn't\": 1, 'sinister beast': 1, 'them very': 1, 'night that': 1, \"wasn't going\": 2, 'my former': 1, 'pretending that': 1, 'all a': 1, 'saint when': 1, 'go on': 1, 'on pretending': 1, 'lindsay she': 1, 'and none': 1, 'let him': 1, 'vile amalgamation': 1, 'this whole': 1, 'night keeping': 1, 'around any': 1, 'cruel cruel': 1, 'contest he': 1, 'sick sick': 1, 'was a': 3, 'wrong he': 1, 'telling stories': 1, 'contest was': 1, 'that he': 1, 'cried all': 1, 'like cattle': 1, 'things like': 1, 'the things': 1, 'keeping up': 1, 'better place': 1, 'things evil': 1, 'crying he': 1, 'what he': 2, 'some saint': 1, 'terrible terrible': 1, 'remember lindsay': 1, 'was stick': 1, 'deal with': 1, 'to live': 1, 'terrible disappointment': 1, 'all up': 1, 'place without': 1, 'off camera': 1, 'who deserved': 1, 'sham an': 1, 'a murderer': 1, 'is a': 2, 'to be': 1, 'her with': 1, 'person i': 1, 'and bad': 1, 'he is': 1, 'that jay': 1, 'stick was': 1, 'the contest': 1, 'like slaves': 1, 'got the': 1, 'for the': 1, 'with him': 1, 'would never': 1, 'any longer': 1, 'is well': 1, 'when all': 1, 'of them': 1, 'fans need': 1, 'he called': 1, 'like animals': 1, 'whole contest': 1, 'deserved every': 1, 'his words': 1, 'and for': 1, 'the villian': 1, 'us the': 1, 'any of': 1, 'live in': 2, 'pushed around': 1, 'all he': 1, 'were all': 1, 'stacksby is': 1, 'killer a': 1, 'joking telling': 1, 'a sinister': 1, 'her such': 1, 'him live': 1, 'one of': 1, 'carrot on': 1, 'a sham': 1, 'nothing compared': 1, 'stories waving': 1, 'she was': 1, 'and feed': 1, 'treated all': 1, 'how wrong': 1, 'of us': 4, 'his home': 1, 'cameras laughing': 1, 'amount to': 1, 'what did': 1, 'his crazy': 1, 'do you': 1, 'compared to': 1, 'last night': 1, 'all of': 2, 'you can': 1, 'and we': 1, 'around like': 1, 'he never': 1, 'cast members': 1, 'and the': 1, 'a sick': 1, 'in his': 2, \"him you've\": 1, \"feed jay's\": 1, 'called her': 1, 'was left': 1, 'his power': 1, 'the contestants': 1, 'the stick': 1, 'things that': 1, 'to shame': 1, 'got what': 1, 'crazy demands': 1, 'sick twisted': 1, 'first to': 1, 'was gone': 1, 'broke her': 1, 'he got': 2, 'animals do': 1, 'can feel': 1, 'more crying': 1, 'man who': 1, 'this you': 1, 'to marry': 1, 'he broke': 1, 'crying crying': 1, 'evil and': 1, 'slaves like': 1, 'and i': 1, 'with his': 2, 'demands and': 1, 'jay was': 1, 'us would': 1, 'like this': 1, 'that was': 1, 'i miss': 1, 'all night': 1, \"i wasn't\": 2, 'camera he': 1, 'speak to': 1, 'murderer but': 1, 'well i': 1, 'very much': 1, 'shame the': 1, 'he would': 1, 'smiling for': 1, 'us last': 1, 'told us': 1, 'your gut': 1, 'the first': 1, 'up all': 1, 'every bit': 1, 'world is': 1, 'a better': 1, 'he told': 1, 'power deal': 1, 'was this': 1, 'him i': 1, 'prop but': 1, 'intended to': 1, 'like a': 1, 'never intended': 1, 'know that': 1, 'us again': 1, 'you may': 1, \"you've seen\": 1, 'none of': 1, 'crying and': 1, 'he deserved': 1, 'think you': 1, 'for what': 1, 'taskmaster he': 1, 'former cast': 1, 'real he': 1, 'had to': 1, 'may call': 1, 'still nothing': 1, 'amalgamation of': 1, 'longer and': 1, 'you remember': 1, 'call me': 1, 'his money': 1, 'was the': 1, 'i showed': 3, 'the world': 1, 'disappointment and': 1, 'go he': 1, 'and more': 1, 'we were': 1, 'right i': 1, 'you know': 2, 'laughing joking': 1, 'elaborate plot': 1, \"it's definitely\": 1, 'beast a': 1, 'a prop': 1, 'an elaborate': 1, 'won the': 1, 'that regardless': 1, 'left was': 1, 'you think': 1, 'gut how': 1, 'words i': 1, 'that we': 1, 'a vile': 1, 'up crying': 1, 'on the': 1, 'bit of': 1, 'be pushed': 1, \"isn't real\": 1, 'was was': 1, 'him smiling': 1, 'home live': 1, 'definitely the': 1, 'sure you': 1, 'need to': 1, 'going to': 2, 'the cameras': 1, 'who won': 1, 'heartless a': 1, \"again it's\": 1, 'him he': 1, 'to any': 1, 'gone all': 1, 'she cried': 1, 'did you': 1, 'of what': 1, \"but i'm\": 1, 'but off': 1, 'cattle like': 1, 'seen him': 1, 'that the': 1, 'cruel taskmaster': 1, 'stick he': 1, 'to go': 1, 'regardless of': 1, 'waving his': 1, 'massive massive': 1, 'he treated': 1, 'and that': 1, 'anything and': 1, 'know him': 1, 'marry one': 1, 'the carrot': 1, 'never speak': 1, 'of all': 1, 'to the': 1, 'that she': 1, 'to know': 1, 'the fans': 1, 'miss my': 1, 'a terrible': 1, 'all that': 1, 'jay stacksby': 1, 'him the': 1, 'plot to': 1, 'villian that': 1, 'in your': 1, 'ego sure': 1, 'showed him': 3, 'a monster': 1, 'all things': 1, 'he was': 3, 'a cruel': 1, 'to let': 1, 'to anything': 1, \"jay's massive\": 1, 'am i': 1, 'would ever': 1, 'me heartless': 1, 'members all': 1, 'of who': 1, 'horrible things': 1, 'him go': 1, 'deserved all': 1, 'the prize': 1}\n"
     ]
    }
   ],
   "source": [
    "#Viewing ngram frequencies for Murder Note Sample\n",
    "print(murderer_sample.ngram_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the owner': 1, 'your favorite': 1, 'owner is': 1, 'and all': 1, 'have fun': 1, 'they gave': 1, 'live without': 1, 'second is': 1, 'are some': 1, \"okay that's\": 1, 'to talk': 1, 'the bio': 1, 'all you': 1, 'with him': 1, 'and spirit': 1, 'actor and': 1, 'bios so': 1, 'get started': 1, 'the cameras': 1, 'definitely pasta': 1, 'you cannot': 1, 'down the': 1, 'one but': 2, 'handsome man': 1, 'cat jerry': 1, 'final question': 1, 'my favorite': 1, 'handsome do': 1, 'them very': 1, 'definitely the': 1, 'of my': 1, 'third i': 1, \"that's it\": 1, 'of art': 1, 'remember seeing': 1, 'does for': 1, 'to you': 1, 'a hard': 1, 'why hmm': 1, 'that man': 1, 'long island': 1, \"them don't\": 1, 'perfect date': 1, 'be the': 2, 'is my': 2, 'our bios': 1, 'the show': 1, 'watching the': 1, 'was my': 1, 'can dance': 1, \"yes it's\": 1, 'jay stacksby': 1, 'do i': 1, 'i do': 1, 'he might': 1, 'what he': 1, 'truly be': 1, 'see what': 1, 'have to': 2, 'best of': 1, 'b jordan': 1, 'stacksby is': 1, 'me in': 1, 'think is': 1, 'dance to': 1, 'the aisle': 1, 'might truly': 1, 'your meal': 1, 'to be': 1, 'cannot live': 1, 'they support': 1, 'think recently': 1, 'doing them': 1, 'it starts': 1, 'east egg': 1, \"recently i'll\": 1, 'up under': 1, 'music you': 1, 'the third': 1, 'of us': 1, \"don't you\": 1, 'for the': 2, 'at a': 1, 'another hard': 1, 'him shirtless': 1, 'and ask': 1, 'jordan every': 1, 'whole body': 1, 'next question': 1, 'my family': 1, 'know jay': 1, 'your perfect': 1, 'to go': 2, 'and best': 1, 'of them': 1, 'use for': 1, 'can feel': 1, 'starts with': 1, 'a warm': 1, 'three things': 1, 'a book': 1, 'feel in': 1, 'okay okay': 1, 'one of': 1, 'delicious but': 1, 'jerry he': 1, 'for our': 1, 'pasta definitely': 1, 'well first': 1, 'i know': 1, 'in everything': 1, \"i can't\": 1, 'is handsome': 1, 'body and': 1, 'you and': 1, 'questionnaire to': 1, 'a nice': 1, 'you know': 1, 'are three': 1, 'do you': 1, 'and they': 1, 'you can': 2, 'all of': 2, 'so lets': 1, 'dishes oh': 1, 'restaurant you': 1, 'date well': 1, 'chores dishes': 1, 'favorite household': 1, 'where the': 1, 'started what': 1, 'ask you': 1, 'i think': 4, 'it okay': 1, 'hate doing': 1, 'with music': 1, 'think he': 1, 'is pasta': 1, 'spirit animal': 1, 'but small': 1, 'one for': 1, 'you who': 1, 'first to': 1, 'to walk': 1, 'so they': 1, 'small restaurant': 1, 'comes out': 1, 'him but': 1, 'pasta and': 1, 'walk down': 1, 'beautiful beautiful': 1, 'of those': 1, 'it is': 1, 'love all': 1, 'lets get': 1, 'to use': 1, \"that's a\": 1, 'trebuchet from': 1, 'us want': 1, \"think i'll\": 1, 'is in': 1, 'with michael': 1, 'cats hiking': 1, 'very much': 1, 'some of': 1, 'in the': 1, 'meal was': 1, 'of that': 1, 'hiking and': 1, 'the first': 1, 'for me': 1, 'well it': 1, 'know like': 1, \"hmm that's\": 1, 'this little': 1, 'of all': 1, 'with a': 2, 'you have': 1, 'i love': 2, 'music music': 1, \"question let's\": 1, 'book so': 1, 'love cats': 1, 'talk to': 1, 'and it': 1, 'a handsome': 1, 'okay final': 1, 'like one': 1, 'dinner at': 1, 'your whole': 1, 'nice dinner': 1, 'little questionnaire': 1, 'is your': 2, 'the back': 1, 'you remember': 1, 'is a': 1, 'support me': 1, 'want to': 1, 'the dishes': 1, \"hi i'm\": 1, 'the one': 1, 'fun watching': 1, 'what is': 1, 'every bit': 1, 'blanket with': 1, \"it's definitely\": 1, 'who is': 1, \"can't believe\": 1, 'shirtless i': 1, 'and curling': 1, 'question what': 1, 'okay next': 1, 'he does': 1, 'under a': 1, 'bit of': 1, 'is electrifying': 1, 'to it': 1, 'island i': 1, 'animal second': 1, 'i hope': 1, 'oh yes': 1, 'handsome handsome': 1, 'beautiful cat': 1, 'form of': 1, 'you how': 1, 'egg long': 1, 'and comes': 1, \"let's see\": 1, 'believe what': 1, 'electrifying and': 1, 'first off': 1, 'what are': 2, 'and why': 1, 'how your': 1, 'art another': 1, 'everything i': 1, \"i'll have\": 2, 'my beautiful': 1, 'family i': 1, 'i just': 1, 'heart and': 1, 'it for': 1, 'hard one': 2, 'favorite form': 1, 'much and': 1, 'bio i': 1, 'hope you': 1, 'just hate': 1, 'cameras okay': 1, 'out to': 1, 'me okay': 1, 'michael b': 1, 'and the': 1, 'seeing him': 1, 'go with': 2, 'without well': 1, 'places where': 1, 'gave this': 1, 'my heart': 1, 'those places': 1, 'things you': 1, 'least favorite': 1, 'he is': 1, 'aisle with': 1, 'man is': 1, 'household chores': 1, 'lily trebuchet': 1, 'in your': 1, 'dishes i': 1, 'man and': 1, 'warm blanket': 1, 'favorite actor': 1, 'my least': 1, 'curling up': 1, 'off my': 1, 'but i': 3, 'a delicious': 1, 'back and': 1, 'from east': 1, \"i'm lily\": 1}\n"
     ]
    }
   ],
   "source": [
    "#Viewing ngram frequencies for Lily's Sample\n",
    "print(lily_sample.ngram_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'myrtle myrtle': 1, 'end of': 1, 'one does': 1, 'called it': 1, 'live without': 1, 'the bike': 1, 'to have': 1, 'doing my': 1, 'taxes i': 1, 'staunch belief': 1, 'be a': 1, 'enjoy reading': 1, 'the competition': 1, 'dates that': 1, 'the opportunity': 1, 'a modicum': 1, 'make and': 1, 'would just': 1, 'i am': 2, 'not have': 1, 'there better': 1, 'to introduce': 1, 'i like': 2, 'because i': 1, 'jay seems': 1, 'measurable effect': 1, 'wanted to': 1, 'a movie': 1, 'every action': 1, 'take a': 1, 'i look': 1, 'i entered': 1, 'destination at': 1, 'effect when': 1, 'the streets': 1, 'competition because': 1, 'the end': 1, 'at the': 1, 'see a': 1, 'no end': 1, 'too whimsical': 1, 'last man': 1, 'dinner he': 1, 'cannot live': 1, 'i want': 2, 'introduce myself': 1, 'plan no': 1, 'and keep': 1, 'money without': 1, 'i cannot': 1, 'my taxes': 1, 'for the': 1, 'when i': 2, 'it a': 1, 'was too': 1, 'thank you': 1, 'he called': 1, 'water emery': 1, 'thinking and': 1, 'to go': 1, 'keep money': 1, 'had no': 1, 'modicum of': 1, 'three things': 1, 'insights that': 1, 'my name': 1, 'want a': 2, 'have a': 1, 'salutations my': 1, 'is my': 1, 'discipline as': 1, 'wandering the': 1, 'times this': 1, 'with insights': 1, 'ride there': 1, '\"walk\" a': 1, 'a serious': 1, 'frivolous at': 1, 'i enjoy': 1, 'a \"walk\"': 2, 'goal sometimes': 1, 'seems frivolous': 1, 'of simple': 1, 'like to': 1, 'does not': 1, 'up wandering': 1, 'simple tastes': 1, 'without having': 1, 'that one': 1, 'to walk': 1, 'my staunch': 1, 'reading thinking': 1, 'forward to': 1, 'not make': 1, 'a bike': 1, 'it is': 1, 'bike ride': 1, 'a woman': 1, 'sometimes we': 1, 'with no': 1, 'away with': 1, 'action i': 1, 'woman of': 1, 'just end': 1, 'this worries': 1, 'walk away': 1, 'me however': 1, 'no destination': 1, 'name myrtle': 1, 'without water': 1, 'to the': 1, 'he wanted': 1, 'path jay': 1, 'i will': 1, 'that i': 1, 'man i': 1, 'a commitment': 1, 'a measurable': 1, 'take to': 1, 'myrtle beech': 1, 'we would': 1, 'however it': 1, 'will now': 1, 'at times': 1, 'beech i': 1, 'a worthy': 1, 'opportunity to': 1, 'have before': 1, 'end goal': 1, 'belief that': 1, 'go on': 1, 'streets after': 1, 'the last': 1, 'before when': 1, 'of the': 1, 'hopeful i': 1, 'now list': 1, 'destination can': 1, 'look forward': 1, 'end up': 1, 'can you': 1, 'better be': 1, 'having a': 1, 'relationship i': 1, 'myself i': 1, 'list three': 1, 'as such': 1, 'after dinner': 1, 'you for': 1, 'you imagine': 1, 'i see': 1, 'that had': 1, 'commitment the': 1, 'worthy destination': 1, 'dated was': 1, 'like every': 1, 'on dates': 1, 'this competition': 1, '\"walk\" with': 1, 'did not': 1, 'imagine i': 1, 'such i': 1, 'am hopeful': 1, 'am a': 1, 'i take': 2, 'bike path': 1, 'no plan': 1, 'dogs thank': 1, 'emery boards': 1, 'movie i': 1, 'i did': 1, 'worries me': 1, 'and doing': 1, 'boards dogs': 1, 'of discipline': 1, 'things i': 1, 'entered this': 1, 'i dated': 1, 'tastes i': 1, 'whimsical he': 1, 'serious relationship': 1}\n"
     ]
    }
   ],
   "source": [
    "#Viewing ngram frequencies for Myrtle's Sample\n",
    "print(myrtle_sample.ngram_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a stunning': 1, 'charming anecdotes': 1, 'beguiles my': 1, 'cannot wait': 1, 'humor when': 1, 'in my': 1, 'with extreme': 1, 'be as': 1, 'enterprise fortune': 1, 'every fiber': 1, 'i am': 2, 'the fishy': 1, 'time to': 1, 'will pierce': 1, 'one of': 1, 'it certainly': 1, 'have the': 1, 'stunning pair': 1, 'a brand': 1, 'i have': 2, 'to you': 1, 'out and': 1, 'paths and': 1, 'blue eyes': 1, 'pierce the': 1, 'life and': 1, 'fishy enterprise': 1, 'that he': 1, 'backyard gardening': 1, 'my countenance': 1, 'i do': 1, 'most curious': 1, 'soul of': 1, 'my every': 1, 'gaze upon': 1, 'what you': 1, 'sense of': 1, 'as he': 1, 'anyone who': 1, 'an adventurous': 1, 'to enjoy': 1, 'that jay': 1, 'greenhouses i': 1, 'about i': 1, 'that beguiles': 1, 'i cannot': 1, 'grand time': 1, 'the backyard': 1, 'he appears': 1, 'wait to': 1, 'of hearts': 1, 'through greenhouses': 1, 'extreme pleasure': 1, 'fishy of': 1, 'compliments of': 1, 'spirit and': 1, 'on long': 1, 'to be': 1, 'young an': 1, 'day life': 1, 'personage i': 1, 'have a': 1, 'he has': 1, 'pleases my': 1, '37 years': 1, 'appears on': 1, 'and short': 1, 'jay will': 1, 'day to': 1, 'television i': 1, 'jaunts through': 1, 'curious tastes': 1, 'i find': 1, 'who dare': 1, 'fortune i': 1, 'adventurous spirit': 1, 'my sense': 1, 'most extraordinary': 1, 'brand new': 1, 'certainly seems': 1, 'love to': 1, 'and humor': 1, 'being on': 1, 'beautiful personage': 1, 'of my': 1, 'jay it': 1, 'a grand': 1, 'the soul': 1, 'you might': 1, 'and vascillates': 1, 'pleasure during': 1, 'and i': 1, 'gregg t': 1, \"i'm fishing\": 1, 'every day': 1, 'enjoy being': 1, 'heart of': 1, 'anecdotes and': 1, 'all i': 1, 'some of': 1, 'and love': 1, 'that instill': 1, 'in the': 1, 'they will': 1, \"when i'm\": 2, 'quite enjoy': 2, 'of course': 1, 'why fishing': 1, 'fiber of': 1, 'i quite': 2, 'interesting as': 1, 'absolutely interesting': 1, 'like a': 1, 'upon my': 1, 'seems like': 1, 'childlike wonder': 1, 'enjoy going': 1, 'hope that': 1, 'during one': 1, 'has some': 1, 'the television': 1, 'gardening and': 1, 'for what': 1, 'new jay': 1, 'explore life': 1, 'of anyone': 1, 'do love': 1, 'long jaunts': 1, 't fishy': 1, 'the fascination': 1, 'my being': 1, 'hearts the': 1, 'ask why': 1, 'of the': 2, 'you all': 1, 'i hope': 1, 'years young': 1, 'hearing tales': 1, 'might ask': 1, 'am gregg': 1, 'on the': 1, 'through garden': 1, 'and about': 1, 'as absolutely': 1, \"i've never\": 1, 'find that': 1, 'countenance i': 1, 'a good': 1, 'time when': 1, 'in style': 1, 'eyes they': 1, 'being scintillates': 1, 'tales that': 1, 'these charming': 1, 'vascillates with': 1, 'lost my': 1, 'of these': 1, 'the most': 2, 'walks through': 1, 'garden paths': 1, 'enjoy hearing': 1, 'fishing for': 2, 'my beautiful': 1, 'extraordinary time': 1, 'life every': 1, 'dare gaze': 1, 'scintillates and': 1, \"and i've\": 1, 'good day': 1, 'pair of': 1, 'wonder i': 1, \"i'm out\": 1, 'fishing fishing': 1, 'instill in': 1, 'on a': 1, 'tastes in': 1, 'fascination that': 1, 'my heart': 1, 'course i': 1, 'radiant blue': 1, 'never lost': 1, 'going on': 1, 'be in': 1, 'of radiant': 1, 'of childlike': 1, 'am 37': 1, 'to explore': 1, 'for compliments': 1, 'and significantly': 1, 'short walks': 1, 'style and': 1, 'significantly pleases': 1, 'will be': 1}\n"
     ]
    }
   ],
   "source": [
    "#Viewing ngram frequencies for Gregg's Sample\n",
    "print(gregg_sample.ngram_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Two Frequency Tables\n",
    "\n",
    "We want to know how similar two frequency tables are, let's write a function that computes the comparison between two frequency tables and scores them based on similarity.\n",
    "\n",
    "Write a function called `frequency_comparison` that takes two parameters, `table1` and `table2`. It should define two local variables, `appearances` and `mutual_appearances`. \n",
    "\n",
    "Iterate through `table1`'s keys and check if `table2` has the same key defined. If it is, compare the two values for the key -- the smaller value should get added to `mutual_appearances` and the larger should get added to `appearances`. If the key doesn't exist in `table2` the value for the key in `table1` should be added to `appearances`.\n",
    "\n",
    "Remember afterwards to iterate through all of `table2`'s keys that aren't in `table1` and add those to `appearances` as well.\n",
    "\n",
    "Return a frequency comparison score equal to the mutual appearances divided by the total appearances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1561822125813449\n",
      "0.16591928251121077\n",
      "0.23196881091617932\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def frequency_comparison(table1, table2):\n",
    "    \n",
    "    #Begin by calculating the appearances and mutual appearances\n",
    "    appearances = 0\n",
    "    mutual_appearances = 0\n",
    "    \n",
    "    for key in table1:\n",
    "        if key in table2:\n",
    "            if table1[key] > table2[key]:\n",
    "                appearances += table1[key]\n",
    "                mutual_appearances += table2[key]\n",
    "            else:\n",
    "                mutual_appearances += table1[key]\n",
    "                appearances += table2[key]\n",
    "        else:\n",
    "            appearances += table1[key]    \n",
    "    for key in table2:\n",
    "        if key not in table1:\n",
    "            appearances += table2[key]\n",
    "        \n",
    "    #Calculate Frequency Comparison Score\n",
    "    freq_comparison_score = mutual_appearances / appearances\n",
    "    return freq_comparison_score\n",
    "\n",
    "print(frequency_comparison(gregg_sample.word_count_frequency, murder_note_table.word_count_frequency))\n",
    "print(frequency_comparison(myrtle_sample.word_count_frequency, murder_note_table.word_count_frequency))\n",
    "print(frequency_comparison(lily_sample.word_count_frequency, murder_note_table.word_count_frequency))\n",
    "print(frequency_comparison(murderer_sample.word_count_frequency, murder_note_table.word_count_frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Average Sentence Length\n",
    "\n",
    "In order to calculate the change between the average sentence lengths of two `TextSamples` we're going to use the formula for the percent difference.\n",
    "\n",
    "Write a function called `percent_difference` that returns the percent difference as calculated from the following formula:\n",
    "\n",
    "$$\\frac{|\\ value1 - value2\\ |}{\\frac{value1 + value2}{2}}$$\n",
    "\n",
    "In the numerator is the absolute value (use `abs()`) of the two values subtracted from each other. In the denominator is the average of the two values (value1 + value2 divided by two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2222222222222222\n",
      "1.6923076923076923\n",
      "0.0\n",
      "0.022222222222222143\n"
     ]
    }
   ],
   "source": [
    "def percent_difference(length1, length2):\n",
    "    percent_diff = (abs(length1 - length2)) / ((length1 + length2) / 2)\n",
    "    return percent_diff\n",
    "\n",
    "print(percent_difference(10, 8))\n",
    "print(percent_difference(1, 12))\n",
    "print(percent_difference(8,8))\n",
    "print(percent_difference(9.1,8.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Similarity with All Three Indicators\n",
    "\n",
    "We want to figure out who did it, so let's use all three of the indicators we built to score text similarity. Define a function `find_text_similarity` that takes two `TextSample` arguments and returns a float between 0 and 1 where 0 means completely different and 1 means the same exact sample. You can evaluate the similarity by the following criteria:\n",
    "\n",
    "- Calculate the percent difference of their average sentence length using `percent_difference`. Save that into a variable called `sentence_length_difference`. Since we want to find how _similar_ the two passages are calculate the inverse of `sentence_length_difference` by using the formula `abs(1 - sentence_length_difference)`. Save that into a variable called `sentence_length_similarity`.\n",
    "- Calculate the difference between their word usage using `frequency_comparison` on both `TextSample`'s `word_count_frequency` attributes. Save that into a variable called `word_count_similarity`.\n",
    "- Calculate the difference between their two-word ngram using `frequency_comparison` on both `TextSample`'s `ngram_frequency` attributes. Save that into a variable called `ngram_similarity`.\n",
    "- Add all three similarities together and divide by 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3245069541617813\n",
      "0.07312907350046245\n",
      "0.2297903530195564\n"
     ]
    }
   ],
   "source": [
    "def find_text_similarity(text1, text2):\n",
    "    \n",
    "    #Calculate Sentence Length Similarity\n",
    "    sentence_length_difference = percent_difference(text1.average_sentence_length, text2.average_sentence_length)\n",
    "    sentence_length_similarity = abs(1 - sentence_length_difference)\n",
    "            \n",
    "    #Calculate Frequency Comparison on Word Count Frequencies\n",
    "    word_count_similarity = frequency_comparison(text1.word_count_frequency, text2.word_count_frequency) \n",
    "        \n",
    "    #Calculate Frequency Comparison on Ngram Frequencies\n",
    "    ngram_similarity = frequency_comparison(text1.ngram_frequency, text2.ngram_frequency)\n",
    "        \n",
    "    #Calculate Overall Similarity Score\n",
    "    similarity_score = (sentence_length_similarity + word_count_similarity + ngram_similarity) / 3\n",
    "    return(similarity_score)\n",
    "    \n",
    "print(find_text_similarity(lily_sample, murder_note_sample))\n",
    "print(find_text_similarity(myrtle_sample, murder_note_sample))\n",
    "print(find_text_similarity(gregg_sample, murder_note_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendering the Results\n",
    "\n",
    "We want to print out the results in a way that we can read! For each contestant on _A Brand New Jay_ print out the following:\n",
    "\n",
    "- Their name\n",
    "- Their similarity score to the murder letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lily Trebuchet has a similarity score of 0.3245069541617813 to the murder letter.\n",
      "Myrtle Beech has a similarity score of 0.07312907350046245 to the murder letter.\n",
      "Gregg T Fishy has a similarity score of 0.2297903530195564 to the murder letter.\n"
     ]
    }
   ],
   "source": [
    "#Printing Similarity Scores for Each Contestant\n",
    "print(lily_sample.author + \" has a similarity score of \" + str(find_text_similarity(lily_sample, murder_note_sample)) + \" to the murder letter.\")\n",
    "print(myrtle_sample.author + \" has a similarity score of \" + str(find_text_similarity(myrtle_sample, murder_note_sample)) + \" to the murder letter.\")\n",
    "print(gregg_sample.author + \" has a similarity score of \" + str(find_text_similarity(gregg_sample, murder_note_sample)) + \" to the murder letter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who Dunnit?\n",
    "\n",
    "In the cell below, print the name of the person who killed Jay Stacksby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is most likely that Lily Trebuchet killed Jay Stacksby. Her intro letter had the highest similarity score to the murder note of any contestant. Her score was 0.3245069541617813\n"
     ]
    }
   ],
   "source": [
    "print(\"It is most likely that \" + lily_sample.author + \" killed Jay Stacksby. Her intro letter had the highest similarity score to the murder note of any contestant. Her score was \" + str(find_text_similarity(lily_sample, murder_note_sample)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
